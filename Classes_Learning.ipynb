{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from sklearn import tree\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_name(phaseid, ds_phase_names):\n",
    "    return ds_phase_names['Phase_englisch'][ds_phase_names['Phase_ID'] == str(phaseid)].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_locations(dataset, ds_stations):\n",
    "    ds_stations.index = ds_stations['Stations_id']\n",
    "    lat = [ds_stations._get_value(row, col) for row, col in zip(dataset['Stations_id'], ['geograph.Breite' for count in range(len(dataset))])] #station_data.lookup(row_labels = dataset['Stations_id'], col_labels = ['geograph.Breite'])\n",
    "    lon = [ds_stations._get_value(row, col) for row, col in zip(dataset['Stations_id'], ['geograph.Laenge' for count in range(len(dataset))])] #station_data._lookup(dataset['Stations_id'], ['geograph.Laenge'])\n",
    "    dataset['lat'] = lat\n",
    "    dataset['lon'] = lon\n",
    "    dataset['lat'] = dataset['lat'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    dataset['lon'] = dataset['lon'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_dependent_response(driver_values, t_dev, responses, thresholds):\n",
    "    #Thresholds are the thresholds in development time where the different growth phases change\n",
    "    #Responses are the response functions, index starting at 'before the first threshold'\n",
    "    #driver values are the inputs to the response function\n",
    "    #t_dev is the (cts) development time\n",
    "    phase = np.digitize(t_dev, thresholds)\n",
    "    response = np.zeros(driver_values.shape)\n",
    "    for phase_index in range(len(responses)):\n",
    "        response += (phase == phase_index)*responses[phase_index](driver_values) #First brackets indicates if we are at the right phase, second takes the response function for each phase\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_stage(phen_data, stage):\n",
    "    return phen_data[phen_data['Name of phase'] == stage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stage_to_stage(phen_data, stage1, stage2, winter_sowing = False):\n",
    "    stage1_frame = isolate_stage(phen_data, stage1)\n",
    "    if winter_sowing: #If the first stage is actually in winter of the previous year, compare the first stage to the year after.\n",
    "        stage1_frame.loc[:, 'Referenzjahr'] = stage1_frame.loc[:, 'Referenzjahr'] + 1\n",
    "    stage1_frame.set_index(['Stations_id', 'Referenzjahr'], inplace=True)\n",
    "    stage2_frame = isolate_stage(phen_data, stage2)\n",
    "    stage2_frame.set_index(['Stations_id', 'Referenzjahr'], inplace=True)\n",
    "    print()\n",
    "    return (stage2_frame['Eintrittsdatum'] - stage1_frame['Eintrittsdatum'])/ pd.to_timedelta(1, unit='D') #.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_xy(x, y, ds):\n",
    "    #Interpolates the input array onto the (non-gridded e.g. phenology station) coordinates x and y.\n",
    "    #Note hyras is not stored on the full grid, but on some kind of subset. Not quite sure how this works. Just got to hope the stations are in a hyras gridpoint.\n",
    "    X_for_interp = xr.DataArray(x, dims=\"modelpoint\")\n",
    "    Y_for_interp = xr.DataArray(y, dims=\"modelpoint\")\n",
    "    return ds.interp(x=X_for_interp, y=Y_for_interp)#, kwargs={\"fill_value\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_to_projection(x_coords, y_coords):\n",
    "    proj_epsg = ccrs.epsg(3034)\n",
    "    proj_latlon = ccrs.PlateCarree()\n",
    "    points_epsg = proj_epsg.transform_points(proj_latlon, x_coords, y_coords)\n",
    "    x_epsg = points_epsg[:, 0]\n",
    "    y_epsg = points_epsg[:, 1]\n",
    "    return x_epsg, y_epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wang_Engel_Temp_response(T, T_min, T_opt, T_max, beta = 1):\n",
    "    alpha = np.log(2)/np.log( (T_max - T_min)/(T_opt - T_min) )\n",
    "    f_T = ( ( (2*(T - T_min)**alpha)*((T_opt - T_min)**alpha) - ((T - T_min)**(2*alpha)) ) / ((T_opt - T_min)**(2*alpha)) )**beta\n",
    "    f_T = np.nan_to_num(f_T)\n",
    "    return f_T*(T >= T_min)*(T<= T_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trapezoid_Temp_response(T, T_min, T_opt1, T_opt2, T_max):\n",
    "    pre_opt = (T>=T_min)*(T<=T_opt1)\n",
    "    opt = (T>=T_opt1)*(T<=T_opt2)\n",
    "    post_opt = (T>=T_opt2)*(T<=T_max)\n",
    "    return pre_opt*(T - T_min)/(T_opt1 - T_min) + opt + post_opt*(T_max - T)/(T_max - T_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlwc1989\\AppData\\Local\\Temp\\ipykernel_23992\\2021697037.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  station_data = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phaenologie_Stationen_Jahresmelder.txt\",sep = \";\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;\", encoding='cp1252', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "class Phenology_set:\n",
    "\n",
    "    phase_names = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phase.txt\", encoding = \"latin1\", engine='python', sep = r';\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;')\n",
    "    station_data = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phaenologie_Stationen_Jahresmelder.txt\",sep = \";\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;\", encoding='cp1252', on_bad_lines='skip')\n",
    "\n",
    "    def __init__(self, address):\n",
    "        self.phen_data = pd.read_csv(address, encoding = \"latin1\", engine='python', sep = r';\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;')\n",
    "        ## CONVERT DATE TO DATETIME ##\n",
    "        self.phen_data['Eintrittsdatum'] = pd.to_datetime(self.phen_data['Eintrittsdatum'], format = '%Y%m%d')\n",
    "        self.phen_data = self.phen_data.drop(self.phen_data[self.phen_data['Qualitaetsniveau'] != 10].index)\n",
    "        self.add_locations()\n",
    "        #print(self.phen_data['Qualitaetsniveau'].values)\n",
    "        self.T_mean = ''\n",
    "        self.GDD_driver_data = ''\n",
    "        self.ordered = False\n",
    "\n",
    "    ### Functions for sorting out dataset ###\n",
    "    def drop_columns(self, drop_list):\n",
    "        for drop_name in drop_list:\n",
    "            self.phen_data = self.phen_data.drop(drop_name, axis = 1)\n",
    "    \n",
    "    def phase_order_name(self, stage_order): #[10, 12, 67, 65, 5, 6, 19, 20, 21, 24, ]\n",
    "        self.phen_data['Order of phase'] = np.nan\n",
    "        self.phen_data['Name of phase'] = ''\n",
    "        for i, phaseid in enumerate(stage_order):\n",
    "            if len(self.phase_names['Phase_englisch'][self.phase_names['Phase_ID'] == str(phaseid)]) != 0:\n",
    "                #print(i, phaseid)\n",
    "                self.phen_data.loc[self.phen_data['Phase_id'] == phaseid, 'Order of phase'] = i\n",
    "                self.phen_data.loc[self.phen_data['Phase_id'] == phaseid, 'Name of phase'] = get_phase_name(phaseid, self.phase_names)\n",
    "        self.order_phen_dataset()\n",
    "\n",
    "    def order_phen_dataset(self):\n",
    "        ## SORT BY TIME ##\n",
    "        if not(np.isin('Order of phase', self.phen_data.columns)):\n",
    "            print('Get phase order and names first')\n",
    "        else:\n",
    "            self.phen_data.sort_values(by = ['Stations_id', 'Referenzjahr', 'Eintrittsdatum', 'Order of phase'])\n",
    "            self.ordered = True\n",
    "    \n",
    "    def get_time_to_next_stage(self):\n",
    "        #Note phen_data must be time and station ordered. Only plots time to next stage - naive as doesn't consider missing phases.\n",
    "        if self.ordered:\n",
    "            ## CALCULATE TIME TO NEXT STAGE ##\n",
    "            self.phen_data['Time to next stage'] = self.phen_data['Eintrittsdatum'].shift(-1) - self.phen_data['Eintrittsdatum']\n",
    "            self.phen_data['Next stage name'] = self.phen_data['Name of phase'].shift(-1)\n",
    "            ## EXCLUDE CHANGES IN STATION ##\n",
    "            self.phen_data.loc[self.phen_data['Stations_id'] != self.phen_data['Stations_id'].shift(-1), 'Time to next stage'] = np.nan\n",
    "            self.phen_data.loc[self.phen_data['Stations_id'] != self.phen_data['Stations_id'].shift(-1), 'Next stage name'] = np.nan\n",
    "        else:\n",
    "            print('Order dataset so I can get time to next stage')\n",
    "\n",
    "    def add_locations(self):\n",
    "        self.phen_data = get_station_locations(self.phen_data, self.station_data)\n",
    "        #LAT, LON = get_station_locations(self.phen_data, self.station_data)\n",
    "        #self.phen_data['lat'] = LAT\n",
    "        #self.phen_data['lon'] = LON\n",
    "        #self.phen_data['lat'] = self.phen_data['lat'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "        #self.phen_data['lon'] = self.phen_data['lon'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    ### Functions for applying GDD model ###\n",
    "    def get_mean_T(self, T_address):\n",
    "        self.T_mean = xr.open_dataset(T_address)\n",
    "\n",
    "    def index_time_from_emergence_day(self):\n",
    "        i_day = self.GDD_driver_data['emergence_dates'].values.copy()\n",
    "        i_daysofyear = np.array([i_day + np.timedelta64(12, 'h') + np.timedelta64(day_of_year, 'D') for day_of_year in range(366)])\n",
    "        time_indexer = xr.DataArray(i_daysofyear, dims=[ \"time\", 'modelpoint'])\n",
    "        self.GDD_driver_data = self.GDD_driver_data.sel(time=time_indexer, method='nearest')\n",
    "\n",
    "    def make_input_array(self):\n",
    "        ## Puts pandas phenological frame into driver xarray and aligns the two\n",
    "        just_emergence_phen_data = self.phen_data.where(self.phen_data['Name of phase'] == 'beginning of emergence').dropna()\n",
    "        ## For now just do data after 2005 to save time\n",
    "        just_emergence_phen_data = just_emergence_phen_data.where(just_emergence_phen_data['Eintrittsdatum'] > np.datetime64('2005-01-01')).dropna()\n",
    "        x_coords = just_emergence_phen_data['lon'].values\n",
    "        y_coords = just_emergence_phen_data['lat'].values\n",
    "        #Makes an array to put into GDD model\n",
    "        print('project to new coords')\n",
    "        x_epsg, y_epsg = latlon_to_projection(x_coords, y_coords)\n",
    "        print('interpolate driver to station locations')\n",
    "        # Working in xarray (not pandas) after this point:\n",
    "        self.GDD_driver_data = interpolate_xy(x_epsg, y_epsg, self.T_mean)\n",
    "        self.GDD_driver_data['emergence_dates'] = ((\"modelpoint\"), just_emergence_phen_data['Eintrittsdatum'].values)\n",
    "        self.GDD_driver_data['Stations_id'] = ((\"modelpoint\"), np.int64(just_emergence_phen_data['Stations_id'].values))\n",
    "        self.GDD_driver_data['Referenzjahr'] = ((\"modelpoint\"), np.int64(just_emergence_phen_data['Referenzjahr'].values))\n",
    "        self.GDD_driver_data = self.GDD_driver_data.assign_coords(modelpoint=np.arange(self.GDD_driver_data.sizes['modelpoint']))\n",
    "        ## Get times to be indexed from emergence day start at emergence day for every site ##\n",
    "        self.index_time_from_emergence_day()\n",
    "        self.GDD_driver_data = self.GDD_driver_data.drop_dims('bnds')\n",
    "        \n",
    "    \n",
    "    def dev_under_response(self, response, driver_variable, maturity_t_dev):\n",
    "        # Response is the rate response to driver values. Driver values are the input to this response. Maturity_t_dev is the t_dev value where we should stop running.\n",
    "        if type(self.GDD_driver_data) == str:\n",
    "            print('Make a dataset for the driving variable first')\n",
    "        else:\n",
    "            t_dev = np.zeros(self.GDD_driver_data[driver_variable].isel(time=0).values.shape) #Continuous development time. When this passes through some thresholds then have change in phase.\n",
    "            dev_time_series = [t_dev.copy()]\n",
    "            for day in range(365):\n",
    "                driver_values = self.GDD_driver_data.isel(time=day)[driver_variable].values \n",
    "                t_dev += response(driver_values, t_dev)\n",
    "                dev_time_series.append(t_dev.copy())\n",
    "            self.model_dev_time_series = np.array(dev_time_series)\n",
    "\n",
    "    def get_phase_dates(self, thresholds):\n",
    "        self.phase_dates_array = np.zeros((len(thresholds), self.model_dev_time_series.shape[1]))\n",
    "        for obs_index in range(self.model_dev_time_series.shape[1]):\n",
    "            self.phase_dates_array[:, obs_index] = np.digitize(thresholds, self.model_dev_time_series[:, obs_index]) #Note that the thresholds are NOT the bins for numpy digitize!\n",
    "    \n",
    "    def get_modelled_dataset(self, phase_list = []):\n",
    "        if len(phase_list) == 0:\n",
    "            computed_phases = [f'modelled time emergence to phase {i + 1}' for i in range(self.phase_dates_array.shape[0])]\n",
    "        else:\n",
    "            computed_phases = [f'modelled time emergence to {phase}' for phase in phase_list]\n",
    "            #Initialize dataset for comparison with station IDs and years\n",
    "        data_comparison = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values),\n",
    "                        'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values),\n",
    "                        }\n",
    "        self.ds_comparison = pd.DataFrame(data_comparison)\n",
    "        #Add modelled phase dates etc. to the comparison set.\n",
    "        for phase_index, phase in enumerate(computed_phases):\n",
    "            self.ds_comparison[phase] = self.phase_dates_array[phase_index, :]\n",
    "        self.ds_comparison.set_index(['Stations_id', 'Referenzjahr'], inplace=True)\n",
    "    \n",
    "    ## Functions for evaluation ##\n",
    "    def get_observed_dataset(self, phase_list, winter_sowing = False):\n",
    "        observed_to_first_stage = time_stage_to_stage(self.phen_data, 'beginning of emergence', phase_list[0], winter_sowing=winter_sowing).dropna()\n",
    "        self.ds_observed = pd.DataFrame({f'observed time emergence to {phase_list[0]}': observed_to_first_stage})\n",
    "        for phase in phase_list[1:]:\n",
    "            self.ds_observed[f'observed time emergence to {phase}'] = time_stage_to_stage(self.phen_data, 'beginning of emergence', phase, winter_sowing=winter_sowing).dropna()\n",
    "        self.ds_observed = self.ds_observed.reset_index()\n",
    "        self.ds_observed = get_station_locations(self.ds_observed, self.station_data)\n",
    "        self.ds_observed = self.ds_observed.set_index(['Referenzjahr', 'Stations_id'])\n",
    "        #LAT, LON = get_station_locations(self.ds_observed, self.station_data)\n",
    "        #self.ds_observed['lat'] = LAT\n",
    "        #self.ds_observed['lon'] = LON\n",
    "        #self.ds_observed['lat'] = self.ds_observed['lat'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "        #self.ds_observed['lon'] = self.ds_observed['lon'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    \n",
    "    def compare_modelled_observed(self):\n",
    "        self.ds_modelled_observed = pd.concat([self.ds_comparison, self.ds_observed], axis = 1)\n",
    "    \n",
    "    #def plot_yearly_obse(self):\n",
    "\n",
    "        \n",
    "\n",
    "    ## Functions for ML ##    \n",
    "    def put_obs_data_in_input_array(self): #, predictor_days = 200\n",
    "        y_data = {}\n",
    "        for phase_time in self.ds_observed.columns:\n",
    "            if phase_time != 'lat' and phase_time != 'lon':\n",
    "                y_data[phase_time] = (['modelpoint'], self.ds_observed[phase_time].values)\n",
    "        print(y_data)\n",
    "        obs_data = xr.Dataset(data_vars = y_data,\n",
    "                                coords = {'Referenzjahr':(['modelpoint'], np.int64(self.ds_observed.index.get_level_values(0))),\n",
    "                                        'Stations_id':(['modelpoint'], np.int64(self.ds_observed.index.get_level_values(1)))}\n",
    "                                        )\n",
    "        print(obs_data)\n",
    "        obs_data = obs_data.set_xindex(['Referenzjahr', 'Stations_id'])\n",
    "        self.GDD_driver_data = self.GDD_driver_data.reset_index('modelpoint')\n",
    "        #self.GDD_driver_data['Stations_id'] = self.GDD_driver_data['Stations_id'].astype(np.int64)\n",
    "        #self.GDD_driver_data['Referenzjahr'] = self.GDD_driver_data['Referenzjahr'].astype(np.int64)\n",
    "        self.GDD_driver_data = self.GDD_driver_data.set_coords(['Referenzjahr', 'Stations_id']) #self.data_for_ML = self.data_for_ML.set_coords(['Referenzjahr', 'Stations_id'])\n",
    "        self.GDD_driver_data = self.GDD_driver_data.set_xindex(['Referenzjahr', 'Stations_id']) #self.data_for_ML = self.data_for_ML.set_xindex(['Referenzjahr', 'Stations_id'])\n",
    "        self.GDD_driver_data = xr.merge([self.GDD_driver_data, obs_data], join='left') #self.data_for_ML = xr.merge([self.data_for_ML, obs_data], join='left')#[driver_variable]\n",
    "        self.GDD_driver_data = self.GDD_driver_data.dropna('modelpoint') #self.data_for_ML = self.data_for_ML.dropna('modelpoint')\n",
    "        \n",
    "    def get_X_y_for_ML(self, driver_variable, predictor_days = 200):\n",
    "        self.X_for_ML = self.GDD_driver_data[driver_variable][:, :predictor_days].values.T\n",
    "        self.y_for_ML = np.array([self.GDD_driver_data[phase_time].values for phase_time in self.ds_observed.columns])[:, :predictor_days].T\n",
    "    \n",
    "    def subsample_X_y(self, subsample_frac = 0.5):\n",
    "        self.subsample = np.random.choice(np.arange(self.y_for_ML.shape[0]),np.int64(np.floor(self.y_for_ML.shape[0]*subsample_frac)))\n",
    "        self.training_X = self.X_for_ML[self.subsample, :]\n",
    "        self.training_y = self.y_for_ML[self.subsample, :]\n",
    "        self.complement_of_subsample = np.delete(np.arange(self.y_for_ML.shape[0]), self.subsample)\n",
    "        self.verification_X = self.X_for_ML[self.complement_of_subsample, :]\n",
    "        self.verification_y = self.y_for_ML[self.complement_of_subsample, :]\n",
    "\n",
    "        self.training_referenzjahr = self.GDD_driver_data['Referenzjahr'].values[self.subsample]\n",
    "        self.training_stationid = self.GDD_driver_data['Stations_id'].values[self.subsample]\n",
    "        self.verification_referenzjahr = self.GDD_driver_data['Referenzjahr'].values[self.complement_of_subsample]\n",
    "        self.verification_stationid = self.GDD_driver_data['Stations_id'].values[self.complement_of_subsample]\n",
    "    \n",
    "    def decision_tree(self, phase_list, md=20):\n",
    "        regr = tree.DecisionTreeRegressor(max_depth=md)\n",
    "        fit = regr.fit(self.training_X, self.training_y)\n",
    "        data_ML_training = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values[self.subsample]),\n",
    "                        'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values[self.subsample]),\n",
    "                        'Training': np.array([True for count in range(len(self.subsample))])\n",
    "                        }\n",
    "        data_ML_verification = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values[self.complement_of_subsample]),\n",
    "                        'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values[self.complement_of_subsample]),\n",
    "                        'Training': np.array([False for count in range(len(self.complement_of_subsample))])\n",
    "                        }\n",
    "        self.ds_ML_predictions_training = pd.DataFrame(data_ML_training)\n",
    "        self.ds_ML_predictions_verification = pd.DataFrame(data_ML_verification)\n",
    "        #Add modelled phase dates etc. to the comparison set.\n",
    "        for phase_index, phase in enumerate(phase_list):\n",
    "            self.ds_ML_predictions_training[f'ML prediction emergence to {phase}'] = fit.predict(self.training_X)[:, phase_index]\n",
    "            self.ds_ML_predictions_verification[f'ML prediction emergence to {phase}'] = fit.predict(self.verification_X)[:, phase_index]\n",
    "            self.ds_ML_predictions_training[f'ML check obs to {phase}'] = self.training_y[:, phase_index]\n",
    "            self.ds_ML_predictions_verification[f'ML check obs to {phase}'] = self.verification_y[:, phase_index]\n",
    "        self.ds_ML_results = pd.concat([self.ds_ML_predictions_verification, self.ds_ML_predictions_training], axis=0)\n",
    "        self.ds_ML_results.set_index(['Stations_id', 'Referenzjahr'], inplace=True)\n",
    "    \n",
    "    def ML_modelled_observed(self):\n",
    "        self.ds_ML_modelled_observed = pd.concat([self.ds_ML_results.drop_duplicates(), self.ds_comparison, self.ds_observed], axis = 1)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project to new coords\n",
      "interpolate driver to station locations\n"
     ]
    }
   ],
   "source": [
    "Maize_set = Phenology_set(\"C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\PH_Jahresmelder_Landwirtschaft_Kulturpflanze_Mais_1936_2023_hist.txt\")\n",
    "Maize_set.drop_columns(['Unnamed: 9'])\n",
    "Maize_set.phase_order_name([10, 12, 67, 65, 5, 6, 19, 20, 21, 24, ])\n",
    "Maize_set.get_mean_T('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\tas_hyras_5_1951_2020_v5-0_de.nc')\n",
    "Maize_set.make_input_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maize_set.ds_observed.xs(19732, level=1, drop_level=False)#[:, 19732]#19914]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE THIS FOR LOOKING AT WHERE ALL THE WHEAT IS\n",
    " - https://atlas.thuenen.de/webspace/agraratlas/agraratlas/index.html?LP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = [lambda x: Wang_Engel_Temp_response(x, 0, 27.5, 40), lambda x: Wang_Engel_Temp_response(x, 0, 33, 44)]\n",
    "thresholds = [45, 65]\n",
    "Maize_set.dev_under_response(lambda x, y: phase_dependent_response(x, y, resps, thresholds), \n",
    "                             'tas', thresholds[-1])\n",
    "Maize_set.get_phase_dates(thresholds)\n",
    "Maize_set.get_observed_dataset(phase_list=['beginning of flowering', 'yellow ripeness'])\n",
    "Maize_set.get_modelled_dataset(phase_list=['beginning of flowering', 'yellow ripeness'])\n",
    "Maize_set.put_obs_data_in_input_array()\n",
    "Maize_set.compare_modelled_observed()\n",
    "Maize_set.get_X_y_for_ML('tas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maize_set.subsample_X_y()\n",
    "Maize_set.decision_tree(['beginning of flowering', 'yellow ripeness'])\n",
    "Maize_set.ML_modelled_observed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_list = ['beginning of emergence', 'beginning of mil ripeness']#['beginning of emergence', 'beginning of flowering', 'beginning of mil ripeness', 'harvest']\n",
    "Maize_set.get_observed_dataset(phase_list=phase_list)\n",
    "plotting.plot_obs_per_year(Maize_set.ds_observed, 'obs_per_year', phase_list=phase_list)\n",
    "plotting.hist2d_locations(Maize_set.ds_observed.loc[2007].dropna()['lat'], Maize_set.ds_observed.loc[2007].dropna()['lon'], bin_num=5)\n",
    "plotting.box_plot_modelled_observed(Maize_set.ds_ML_modelled_observed.where(Maize_set.ds_ML_modelled_observed['Training'] ==False), ['beginning of flowering', 'yellow ripeness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ML_dataset(self, phase_list = []):\n",
    "    if len(phase_list) == 0:\n",
    "        computed_phases = [f'modelled time emergence to phase {i + 1}' for i in range(self.phase_dates_array.shape[0])]\n",
    "    else:\n",
    "        computed_phases = [f'modelled time emergence to {phase}' for phase in phase_list]\n",
    "        #Initialize dataset for comparison with station IDs and years\n",
    "    data_comparison = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values),\n",
    "                    'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values),\n",
    "                    }\n",
    "    self.ds_comparison = pd.DataFrame(data_comparison)\n",
    "    #Add modelled phase dates etc. to the comparison set.\n",
    "    ML_e\n",
    "    for phase_index, phase in enumerate(computed_phases):\n",
    "        self.ds_comparison[phase] = self.ML_fit[phase_index, :]\n",
    "    self.ds_comparison.set_index(['Stations_id', 'Referenzjahr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.26627311 -0.26627314  0.30000001 ...  2.14231484  4.09141191\n",
      "  3.53372684] [-2.26627311 -0.26627314  0.30000001 ...  2.14231484  4.09141191\n",
      "  3.53372684]\n",
      "100.0 100.0\n"
     ]
    }
   ],
   "source": [
    "## Check that indexing is right\n",
    "S = 7504\n",
    "Y = 2005\n",
    "print(ds['tas'].sel(Referenzjahr=Y, Stations_id = S).values, Maize_set.GDD_driver_data['tas'].sel(Referenzjahr=Y, Stations_id = S).values)\n",
    "print(ds.sel(Referenzjahr=Y, Stations_id = S)['observed time emergence to beginning of flowering'].values, Maize_set.ds_observed.loc[(S, Y)]['observed time emergence to beginning of flowering'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phenology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
