{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import algopy\n",
    "#from algopy import UTPM, exp\n",
    "import copy\n",
    "import calendar\n",
    "#import seaborn as sns\n",
    "\n",
    "import plotting\n",
    "import dataset_fctns\n",
    "import modelling_fctns\n",
    "import seaborn as sns\n",
    "#from dwd_phenpy import Phenology_set\n",
    "\n",
    "#import openeo\n",
    "#import ee\n",
    "## Trigger the authentication flow.\n",
    "#ee.Authenticate()#(auth_mode='localhost')\n",
    "# Initialize the library.\n",
    "#ee.Initialize(project='ee-martinparker637')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlwc1989\\AppData\\Local\\Temp\\ipykernel_24612\\1371601084.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  station_data = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phaenologie_Stationen_Jahresmelder.txt\",sep = \";\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;\", encoding='cp1252', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "class Phenology_set:\n",
    "\n",
    "    phase_names = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phase.txt\", encoding = \"latin1\", engine='python', sep = r';\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;')\n",
    "    station_data = pd.read_csv(\"https://opendata.dwd.de/climate_environment/CDC/help/PH_Beschreibung_Phaenologie_Stationen_Jahresmelder.txt\",sep = \";\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;\", encoding='cp1252', on_bad_lines='skip')\n",
    "    \n",
    "    def __init__(self, address, raw = False, dwd_data = True):\n",
    "        if raw:\n",
    "            self.phen_data = pd.read_csv(address, encoding = \"latin1\", engine='python', sep = r';\\s+|;\\t+|;\\s+\\t+|;\\t+\\s+|;|\\s+;|\\t+;|\\s+\\t+;|\\t+\\s+;')\n",
    "        else:\n",
    "            self.phen_data = pd.read_csv(address)\n",
    "        ## CONVERT DATE TO DATETIME ##\n",
    "        if dwd_data:\n",
    "            self.phen_data['Eintrittsdatum'] = pd.to_datetime(self.phen_data['Eintrittsdatum'], format = '%Y%m%d')\n",
    "            self.phen_data = self.phen_data.drop(self.phen_data[self.phen_data['Qualitaetsniveau'] != 10].index)\n",
    "            self.add_locations()\n",
    "        self.phase_list = [] #list of phases to consider\n",
    "        #print(self.phen_data['Qualitaetsniveau'].values)\n",
    "        self.T_mean = ''\n",
    "        self.GDD_driver_data = ''\n",
    "        self.ordered = False\n",
    "        self.first_input_array = True\n",
    "\n",
    "    ### Functions for sorting out dataset ###\n",
    "    def drop_columns(self, drop_list):\n",
    "        for drop_name in drop_list:\n",
    "            try:\n",
    "                self.phen_data = self.phen_data.drop(drop_name, axis = 1)\n",
    "            except:\n",
    "                print(f'Column {drop_name} not found')\n",
    "                continue\n",
    "    \n",
    "    def phase_order_name(self, stage_order): #[10, 12, 67, 65, 5, 6, 19, 20, 21, 24, ]\n",
    "        self.phen_data['Order of phase'] = np.nan\n",
    "        self.phen_data['Name of phase'] = ''\n",
    "        for i, phaseid in enumerate(stage_order):\n",
    "            if len(self.phase_names['Phase_englisch'][self.phase_names['Phase_ID'] == str(phaseid)]) != 0:\n",
    "                #print(i, phaseid)\n",
    "                self.phen_data.loc[self.phen_data['Phase_id'] == phaseid, 'Order of phase'] = i\n",
    "                self.phen_data.loc[self.phen_data['Phase_id'] == phaseid, 'Name of phase'] = dataset_fctns.get_phase_name(phaseid, self.phase_names)\n",
    "        self.order_phen_dataset()\n",
    "\n",
    "    def order_phen_dataset(self):\n",
    "        ## SORT BY TIME ##\n",
    "        if not(np.isin('Order of phase', self.phen_data.columns)):\n",
    "            print('Get phase order and names first')\n",
    "        else:\n",
    "            self.phen_data.sort_values(by = ['Stations_id', 'Referenzjahr', 'Eintrittsdatum', 'Order of phase'])\n",
    "            self.ordered = True\n",
    "    \n",
    "    def get_time_to_next_stage(self):\n",
    "        #Note phen_data must be time and station ordered. Only plots time to next stage - naive as doesn't consider missing phases.\n",
    "        if self.ordered:\n",
    "            ## CALCULATE TIME TO NEXT STAGE ##\n",
    "            self.phen_data['Time to next stage'] = self.phen_data['Eintrittsdatum'].shift(-1) - self.phen_data['Eintrittsdatum']\n",
    "            self.phen_data['Next stage name'] = self.phen_data['Name of phase'].shift(-1)\n",
    "            ## EXCLUDE CHANGES IN STATION ##\n",
    "            self.phen_data.loc[self.phen_data['Stations_id'] != self.phen_data['Stations_id'].shift(-1), 'Time to next stage'] = np.nan\n",
    "            self.phen_data.loc[self.phen_data['Stations_id'] != self.phen_data['Stations_id'].shift(-1), 'Next stage name'] = np.nan\n",
    "        else:\n",
    "            print('Order dataset so I can get time to next stage')\n",
    "\n",
    "    def add_locations(self):\n",
    "        self.phen_data = dataset_fctns.get_station_locations(self.phen_data, self.station_data)\n",
    "        #LAT, LON = dataset_fctns.get_station_locations(self.phen_data, self.station_data)\n",
    "        #self.phen_data['lat'] = LAT\n",
    "        #self.phen_data['lon'] = LON\n",
    "        #self.phen_data['lat'] = self.phen_data['lat'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "        #self.phen_data['lon'] = self.phen_data['lon'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    ### Functions for applying GDD model ###\n",
    "    def get_mean_T(self, T_address):\n",
    "        self.T_mean = xr.open_dataset(T_address)\n",
    "\n",
    "    def index_time_from_emergence_day(self):\n",
    "        i_day = self.GDD_driver_data['emergence_dates'].values.copy()\n",
    "        i_daysofyear = np.array([i_day + np.timedelta64(12, 'h') + np.timedelta64(day_of_year, 'D') for day_of_year in range(366)])\n",
    "        time_indexer = xr.DataArray(i_daysofyear, dims=[ \"time\", 'modelpoint'])\n",
    "        self.GDD_driver_data = self.GDD_driver_data.sel(time=time_indexer, method='nearest')\n",
    "\n",
    "    def align_emergence_obs_with_driver_data(self):\n",
    "        ## Make sure we are comparing to observations where we have the driver data;\n",
    "        #1. Align the times - need to check as it might run for some days then go off the end.\n",
    "        #self.just_emergence = self.just_emergence.where(self.just_emergence['Referenzjahr'] <= 2024)\n",
    "        ## Make sure all elements are in the driver data\n",
    "        self.just_emergence = self.just_emergence.loc[np.isin(self.just_emergence['Eintrittsdatum'] + np.timedelta64(12, 'h'), self.GDD_driver_data['time'])]\n",
    "        self.just_emergence = self.just_emergence.loc[np.isin(self.just_emergence['Stations_id'], self.GDD_driver_data['Stations_id'])]\n",
    "        self.just_emergence = self.just_emergence.dropna()\n",
    "    \n",
    "    def get_unique_xy_station(self, x_coords, y_coords, station_ids):\n",
    "        unique_values = np.unique(np.stack([x_coords, y_coords, station_ids]), axis = 1)\n",
    "        return unique_values[0, :], unique_values[1, :], unique_values[2, :]\n",
    "\n",
    "    def make_input_array(self, epsg_num = 3035, latlon_proj = False):\n",
    "        self.latlon_proj = latlon_proj\n",
    "        ## Puts pandas phenological frame into driver xarray and aligns the two\n",
    "        #self.just_emergence = self.phen_data.where(self.phen_data['Name of phase'] == 'beginning of emergence').dropna()\n",
    "        ## For now just do data after 2005 to save time\n",
    "        if self.first_input_array:\n",
    "            self.obs_for_GDD = self.phen_data.where(self.phen_data['Eintrittsdatum'] >= np.datetime64('2001-01-01')).dropna()\n",
    "            x_coords = self.obs_for_GDD['lon'].values\n",
    "            y_coords = self.obs_for_GDD['lat'].values\n",
    "            station_ids = np.int64(self.obs_for_GDD['Stations_id'].values)\n",
    "            x_unique, y_unique, stations = self.get_unique_xy_station(x_coords, y_coords, station_ids)\n",
    "            #Makes an array to put into GDD model\n",
    "            print('project to new coords')\n",
    "            self.stations = stations\n",
    "            if not(latlon_proj):\n",
    "                x_epsg, y_epsg = dataset_fctns.latlon_to_projection(x_unique, y_unique, epsg_num = epsg_num)\n",
    "                self.x_driver_proj = x_epsg\n",
    "                self.y__driver_proj = y_epsg\n",
    "            else:\n",
    "                self.x_driver_proj = x_unique\n",
    "                self.y__driver_proj = y_unique\n",
    "            print('interpolate driver to station locations')\n",
    "            # Working in xarray (not pandas) after this point:\n",
    "            #print('Latlonproj:', not(latlon_proj))\n",
    "            self.GDD_driver_data = dataset_fctns.interpolate_xy(self.x_driver_proj, self.y__driver_proj, self.T_mean, xy=not(latlon_proj))\n",
    "            self.GDD_driver_data = self.GDD_driver_data.assign_coords(Stations_id=(\"modelpoint\", self.stations))\n",
    "            if not(latlon_proj):\n",
    "                self.GDD_driver_data = self.GDD_driver_data.drop_dims('bnds')\n",
    "            self.GDD_driver_data = self.GDD_driver_data.set_xindex(['Stations_id'])\n",
    "            self.first_input_array = False\n",
    "        else: \n",
    "            new_GDD_driver_data = dataset_fctns.interpolate_xy(self.x_driver_proj, self.y__driver_proj, self.T_mean, xy=not(latlon_proj))\n",
    "            new_GDD_driver_data = new_GDD_driver_data.assign_coords(Stations_id=(\"modelpoint\", self.stations))\n",
    "            if not(latlon_proj):\n",
    "                new_GDD_driver_data = self.GDD_driver_data.drop_dims('bnds')\n",
    "            new_GDD_driver_data = new_GDD_driver_data.set_xindex(['Stations_id'])\n",
    "            self.GDD_driver_data = xr.concat([self.GDD_driver_data, new_GDD_driver_data], dim='time')\n",
    "            self.GDD_driver_data = self.GDD_driver_data.sortby('time')\n",
    "\n",
    "    def dev_under_response(self, response, driver_variable, maturity_t_dev):\n",
    "        # Response is the rate response to driver values. Driver values are the input to this response. Maturity_t_dev is the t_dev value where we should stop running.\n",
    "        self.obs_for_GDD = self.obs_for_GDD.where(self.obs_for_GDD['Referenzjahr'] <= 2023)\n",
    "        ## Make the indexer to extract things at the right time.\n",
    "        #self.align_emergence_obs_with_driver_data()\n",
    "        self.obs_for_GDD = dataset_fctns.add_SOS_to_df(self.obs_for_GDD)\n",
    "        self.obs_for_GDD['WC SOS date'] = pd.to_datetime(self.obs_for_GDD['Referenzjahr'], format='%Y') + pd.to_timedelta(self.obs_for_GDD['SOS'], 'D')\n",
    "        time_station = xr.Dataset.from_dataframe(self.obs_for_GDD[['Stations_id', 'WC SOS date']])\n",
    "        time_station = time_station.rename({'index':'Emergence observation', 'WC SOS date':'time'})\n",
    "        if not(self.latlon_proj):\n",
    "            time_station['time'] += np.timedelta64(12, 'h')\n",
    "        ## Initiate development time storage object.\n",
    "        t_dev = np.zeros(time_station.sizes['Emergence observation']) #Continuous development time. When this passes through some thresholds then have change in phase.\n",
    "        dev_time_series = [t_dev.copy()]\n",
    "        ## Make sure driver dataset uses station id to index this dimension\n",
    "        try:\n",
    "            self.GDD_driver_data = self.GDD_driver_data.set_xindex(['Stations_id'])\n",
    "        except:\n",
    "            print('Couldn\\'t reset index for station')\n",
    "        #Run model\n",
    "        for day in range(300):\n",
    "            #print(day)\n",
    "            driver_values = self.GDD_driver_data.sel(time_station)[driver_variable].values \n",
    "            t_dev += response(driver_values, t_dev)\n",
    "            dev_time_series.append(t_dev.copy())\n",
    "            time_station['time'] += np.timedelta64(1, 'D')\n",
    "        dev_time_series.append(self.obs_for_GDD['Eintrittsdatum'].values.astype('datetime64[Y]'))\n",
    "        dev_time_series.append(self.obs_for_GDD['Stations_id'].values)\n",
    "        self.model_dev_time_series = np.array(dev_time_series)\n",
    "        self.GDD_driver_data['Development Time'] = (('days from emergence', 'Emergence observation'), self.model_dev_time_series)\n",
    "\n",
    "    def get_phase_dates(self, thresholds):\n",
    "        column_names = np.concatenate([np.array(thresholds), ['Referenzjahr'], ['Stations_id']])\n",
    "        self.phase_dates_array = np.zeros((len(thresholds), self.model_dev_time_series.shape[1]))\n",
    "        for obs_index in range(self.model_dev_time_series.shape[1]):\n",
    "            self.phase_dates_array[:, obs_index] = np.digitize(thresholds, self.model_dev_time_series[:-2, obs_index].astype(np.float64))\n",
    "        self.phase_dates_array = np.concatenate([self.phase_dates_array, [pd.to_datetime(self.model_dev_time_series[-2]).year], [self.model_dev_time_series[-1]]], axis=0)\n",
    "        self.phase_dates_array = pd.DataFrame(self.phase_dates_array.T, columns = column_names)\n",
    "        self.phase_dates_array.set_index(['Referenzjahr', 'Stations_id'])\n",
    "        self.phase_dates_calculated = True\n",
    "        \n",
    "        #Note that the thresholds are NOT the bins for numpy digitize!\n",
    "    \n",
    "    ## Functions for evaluation ##\n",
    "    def get_observed_dataset(self, winter_sowing = False, count_from_SOS = True):\n",
    "        if count_from_SOS:\n",
    "            self.phen_data = dataset_fctns.add_SOS_to_df(self.phen_data)\n",
    "            self.phen_data['WC SOS date'] = pd.to_datetime(self.phen_data['Referenzjahr'], format='%Y') + pd.to_timedelta(self.phen_data['SOS'], 'D')\n",
    "            self.ds_observed = self.phen_data[['Stations_id', 'Referenzjahr', 'lat', 'lon', 'WC SOS date']].drop_duplicates()\n",
    "            for phase in self.phase_list:\n",
    "                just_phase = self.phen_data.loc[self.phen_data['Name of phase'] == phase]\n",
    "                just_phase= just_phase.assign(**{f'observed time to {phase}': just_phase['Eintrittsdatum'] - just_phase['WC SOS date']})\n",
    "                self.ds_observed = self.ds_observed.merge(just_phase[[f'observed time to {phase}', 'Referenzjahr', 'Stations_id']], how = 'left', on = ['Referenzjahr', 'Stations_id'])\n",
    "        else:\n",
    "            observed_to_first_stage = dataset_fctns.time_stage_to_stage(self.phen_data, 'beginning of emergence', self.phase_list[0], winter_sowing=winter_sowing).dropna()\n",
    "            self.ds_observed = pd.DataFrame({f'observed time to {self.phase_list[0]}': observed_to_first_stage})\n",
    "            for phase in self.phase_list[1:]:\n",
    "                self.ds_observed[f'observed time to {phase}'] = dataset_fctns.time_stage_to_stage(self.phen_data, 'beginning of emergence', phase, winter_sowing=winter_sowing).dropna()\n",
    "            self.ds_observed = self.ds_observed.reset_index()\n",
    "            self.ds_observed = dataset_fctns.get_station_locations(self.ds_observed, self.station_data)\n",
    "            self.ds_observed = self.ds_observed.merge(self.obs_for_GDD[['Eintrittsdatum', 'Referenzjahr', 'Stations_id']], how = 'outer', on=['Referenzjahr', 'Stations_id']).rename(columns={'Eintrittsdatum':'emergence date'})\n",
    "        #self.ds_observed = self.ds_observed.set_index(['Referenzjahr', 'Stations_id'])\n",
    "        #self.ds_observed = pd.concat([self.just_emergence.set_index(['Referenzjahr', 'Stations_id'], inplace=False)['Eintrittsdatum'], self.ds_observed], axis=1).rename(columns={'Eintrittsdatum':'emergence date'})\n",
    "        #LAT, LON = dataset_fctns.get_station_locations(self.ds_observed, self.station_data)\n",
    "        #self.ds_observed['lat'] = LAT\n",
    "        #self.ds_observed['lon'] = LON\n",
    "        #self.ds_observed['lat'] = self.ds_observed['lat'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "        #self.ds_observed['lon'] = self.ds_observed['lon'].map(lambda x: x[0] if isinstance(x, np.float64) == False else x)\n",
    "    \n",
    "    def compare_modelled_observed(self):\n",
    "        self.ds_modelled_observed = pd.merge(self.ds_observed, self.phase_dates_array, how='outer', on=['Referenzjahr', 'Stations_id'])\n",
    "\n",
    "    def get_X_y_for_ML2(self, driver_variable = 't2m'):\n",
    "        self.observations_to_use = self.ds_observed[['Stations_id', 'Referenzjahr', 'WC SOS date']].where(self.ds_observed['Referenzjahr'] >= 2001).dropna().drop_duplicates()\n",
    "        # make an indexing array to pull values from the array of temperatures\n",
    "        time_station = xr.Dataset.from_dataframe(self.observations_to_use)\n",
    "        time_station = time_station.rename({'index':'observation', 'WC SOS date':'time'})\n",
    "        #print(time_station)\n",
    "        if not(self.latlon_proj):\n",
    "            time_station['time'] += np.timedelta64(12, 'h') - np.timedelta64(60, 'D')\n",
    "\n",
    "        ## Initiate development time storage object - a list with a value for all the stations, that will change over time and be stored in a list.\n",
    "        t_dev = np.zeros(time_station.sizes['observation']) #Continuous development time. When this passes through some thresholds then have change in phase.\n",
    "        dev_time_series = [t_dev.copy()]\n",
    "        ## Make sure driver dataset uses station id to index this dimension\n",
    "        try:\n",
    "            self.GDD_driver_data = self.GDD_driver_data.set_xindex(['Stations_id'])\n",
    "        except:\n",
    "            print('Couldn\\'t reset index for station')\n",
    "        \n",
    "        #Run model\n",
    "        for day in range(310):\n",
    "            # Pull values for temperature out of data frame\n",
    "            driver_values = self.GDD_driver_data.sel(time_station[['Stations_id', 'time']])#[driver_variable]#.values \n",
    "            driver_frame_at_day = driver_values[[driver_variable, 'Stations_id', 'time']].to_pandas().reset_index().drop(['number', 'lon', 'lat', 'observation'], axis=1)\n",
    "            driver_frame_at_day['Referenzjahr'] = driver_frame_at_day['time'].dt.year\n",
    "            driver_frame_at_day = driver_frame_at_day.drop('time', axis=1)\n",
    "            driver_frame_at_day = driver_frame_at_day.rename(columns = {driver_variable:f'{driver_variable} at day {day}'})\n",
    "            self.observations_to_use = self.observations_to_use.merge(driver_frame_at_day, on=['Referenzjahr', 'Stations_id'])\n",
    "            time_station['time'] += np.timedelta64(1, 'D')\n",
    "        self.driver_frame_for_ML = self.observations_to_use.merge(self.ds_observed[['Referenzjahr', 'Stations_id'] + [f'observed time to {phase}' for phase in self.phase_list]]).drop_duplicates(subset = ['Referenzjahr', 'Stations_id'])\n",
    "\n",
    "    def get_X_y_for_ML(self, driver_variable = 'tas', predictor_days = 200, cumulative = False, thinning_parameter = 1, start_year = 2020, end_year = 2023):\n",
    "        self.just_emergence = dataset_fctns.add_EOS_to_df(self.just_emergence)\n",
    "        self.just_emergence = dataset_fctns.add_SOS_to_df(self.just_emergence)\n",
    "        self.just_emergence['WC SOS date'] = pd.to_datetime(self.just_emergence['Referenzjahr'], format='%Y') + pd.to_timedelta(self.just_emergence['SOS'], 'D')\n",
    "        self.just_emergence['SOS'] = pd.to_timedelta(self.just_emergence['SOS'], 'D')\n",
    "        time_station = xr.Dataset.from_dataframe(self.just_emergence[['Stations_id', 'SOS']].drop_duplicates()) #, 'Referenzjahr'\n",
    "        time_station = time_station.set_coords('Stations_id').set_xindex(['Stations_id'])\n",
    "        time_station = time_station.drop_vars('index')\n",
    "        time_station = time_station.expand_dims(dim={'time':pd.to_timedelta(np.arange(0, predictor_days), 'D')})\n",
    "        time_station = time_station.expand_dims(dim={'Referenzjahr':pd.date_range(f'{start_year}-01-01', periods = end_year - start_year, freq='YS')})\n",
    "        time_station['SOS'] = time_station['SOS'] + time_station['Referenzjahr'] + time_station['time'] \n",
    "        if not(self.latlon_proj):\n",
    "            time_station['SOS'] += pd.Timedelta(12, 'h')\n",
    "        time_station = time_station.rename({'time':'time_from_SOS', 'SOS':'time'})\n",
    "        time_station = time_station.reset_index('Stations_id').reset_coords(names = 'Stations_id')\n",
    "        self.time_station = time_station\n",
    "        self.driver_data_for_ML = self.GDD_driver_data[driver_variable].sel(time_station)\n",
    "        self.driver_data_for_ML = self.driver_data_for_ML.rename({'index': 'Stations_id'})\n",
    "        self.driver_data_for_ML = self.driver_data_for_ML.set_xindex(['Stations_id'])\n",
    "        self.driver_data_for_ML['Referenzjahr'] = pd.to_datetime(self.driver_data_for_ML['Referenzjahr']).year\n",
    "        self.driver_frame_for_ML = self.driver_data_for_ML.to_dataframe(dim_order = ['Referenzjahr', 'Stations_id', 'time_from_SOS'])\n",
    "        self.driver_frame_for_ML = pd.concat([self.driver_frame_for_ML[driver_variable].unstack(),\n",
    "                                            self.driver_frame_for_ML['lat'].unstack()['0 days'].rename('lat'),\n",
    "                                            self.driver_frame_for_ML['lon'].unstack()['0 days'].rename('lon'),\n",
    "                                            self.driver_frame_for_ML['time'].unstack()['0 days'].rename('WC SOS')], axis=1)\n",
    "        self.driver_frame_for_ML.rename(columns={self.driver_frame_for_ML.columns[x]: f'{driver_variable} day {x}' for x in range(200)}, inplace=True)\n",
    "        self.driver_frame_for_ML = pd.merge(self.driver_frame_for_ML.reset_index(), self.ds_observed, how='left', on=['Referenzjahr', 'Stations_id'], suffixes=(None, '_observed')).drop(['lat_observed', 'lon_observed'], axis = 1)\n",
    "        if self.phase_dates_calculated:\n",
    "            self.driver_frame_for_ML = pd.merge(self.driver_frame_for_ML, self.phase_dates_array.reset_index(), how='left', on=['Referenzjahr', 'Stations_id'])\n",
    "\n",
    "    def subsample_X_y(self, subsample_frac = 0.5):\n",
    "        self.subsample = np.random.choice(np.arange(self.y_for_ML.shape[0]),np.int64(np.floor(self.y_for_ML.shape[0]*subsample_frac)))\n",
    "        self.training_X = self.X_for_ML[self.subsample, :]\n",
    "        self.training_y = self.y_for_ML[self.subsample, :]\n",
    "        self.complement_of_subsample = np.delete(np.arange(self.y_for_ML.shape[0]), self.subsample)\n",
    "        self.verification_X = self.X_for_ML[self.complement_of_subsample, :]\n",
    "        self.verification_y = self.y_for_ML[self.complement_of_subsample, :]\n",
    "\n",
    "        self.training_referenzjahr = self.GDD_driver_data['Referenzjahr'].values[self.subsample]\n",
    "        self.training_stationid = self.GDD_driver_data['Stations_id'].values[self.subsample]\n",
    "        self.verification_referenzjahr = self.GDD_driver_data['Referenzjahr'].values[self.complement_of_subsample]\n",
    "        self.verification_stationid = self.GDD_driver_data['Stations_id'].values[self.complement_of_subsample]\n",
    "    \n",
    "    def decision_tree(self, md=20):\n",
    "        self.regr = tree.DecisionTreeRegressor(max_depth=md, min_samples_leaf=5)\n",
    "        self.fit = self.regr.fit(self.training_X, self.training_y)\n",
    "        data_ML_training = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values[self.subsample]),\n",
    "                        'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values[self.subsample]),\n",
    "                        'Training': np.array([True for count in range(len(self.subsample))])\n",
    "                        }\n",
    "        data_ML_verification = {'Stations_id': np.int64(self.GDD_driver_data['Stations_id'].values[self.complement_of_subsample]),\n",
    "                        'Referenzjahr': np.int64(self.GDD_driver_data['Referenzjahr'].values[self.complement_of_subsample]),\n",
    "                        'Training': np.array([False for count in range(len(self.complement_of_subsample))])\n",
    "                        }\n",
    "        self.ds_ML_predictions_training = pd.DataFrame(data_ML_training)\n",
    "        self.ds_ML_predictions_verification = pd.DataFrame(data_ML_verification)\n",
    "        #Add modelled phase dates etc. to the comparison set.\n",
    "        for phase_index, phase in enumerate(self.phase_list):\n",
    "            self.ds_ML_predictions_training[f'ML prediction emergence to {phase}'] = self.fit.predict(self.training_X)[:, phase_index]\n",
    "            self.ds_ML_predictions_verification[f'ML prediction emergence to {phase}'] = self.fit.predict(self.verification_X)[:, phase_index]\n",
    "            self.ds_ML_predictions_training[f'ML check obs to {phase}'] = self.training_y[:, phase_index]\n",
    "            self.ds_ML_predictions_verification[f'ML check obs to {phase}'] = self.verification_y[:, phase_index]\n",
    "        self.ds_ML_predictions_training = self.ds_ML_predictions_training.drop_duplicates()\n",
    "        self.ds_ML_predictions_verification = self.ds_ML_predictions_verification.drop_duplicates()\n",
    "        self.ds_ML_results = pd.concat([self.ds_ML_predictions_verification, self.ds_ML_predictions_training], axis=0)\n",
    "        self.ds_ML_results.set_index(['Referenzjahr', 'Stations_id'], inplace=True)\n",
    "    \n",
    "    def ML_modelled_observed(self):\n",
    "        self.ds_ML_modelled_observed = pd.concat([self.ds_ML_results, self.ds_comparison, self.ds_observed], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "phen_data_CIMMYT = pd.read_csv('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\African_data\\\\with_SOS\\\\CIMMYT_phen_data.csv')\n",
    "phen_data_CIMMYT = dataset_fctns.columns_to_datetime(phen_data_CIMMYT, ['PlantingDate', 'AnthesisDate', 'DaysToSilk'])\n",
    "phen_data_CIMMYT = phen_data_CIMMYT.rename(columns={'AnthesisDate': 'observed time to beginning of flowering'})\n",
    "phen_data_CIMMYT = dataset_fctns.prepare_African_phen_ds(phen_data_CIMMYT, 'beginning of flowering')#'PlantingDate', \n",
    "phen_data_CIMMYT = phen_data_CIMMYT.loc[(phen_data_CIMMYT['observed time to beginning of flowering'] - phen_data_CIMMYT['WC SOS date']).dt.days < 240]\n",
    "phen_data_CIMMYT['WC SOS date'] = phen_data_CIMMYT['PlantingDate']\n",
    "phen_data_CIMMYT['Referenzjahr'] = phen_data_CIMMYT['yrcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project to new coords\n",
      "interpolate driver to station locations\n"
     ]
    }
   ],
   "source": [
    "Maize_set_Africa = Phenology_set('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\African_data\\\\Lobell2011\\\\maizedata.lobell.sep2011.csv', raw = False, dwd_data=False)\n",
    "Maize_set_Africa.phen_data = phen_data_CIMMYT\n",
    "Maize_set_Africa.get_mean_T('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\ERA5\\\\Africa\\\\ERA5_SSA_1999_2009.nc')\n",
    "Maize_set_Africa.phen_data['Eintrittsdatum'] = Maize_set_Africa.phen_data['PlantingDate']\n",
    "#Maize_set_Africa.phen_data['Stations_id'] = Maize_set_Africa.phen_data['sitecode']\n",
    "Maize_set_Africa.make_input_array(latlon_proj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Unnamed: 9 not found\n",
      "project to new coords\n",
      "interpolate driver to station locations\n"
     ]
    }
   ],
   "source": [
    "#Maize_set = Phenology_set(\"C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\PH_Jahresmelder_Landwirtschaft_Kulturpflanze_Mais_1936_2023_hist.txt\", raw = True)\n",
    "Maize_set = Phenology_set('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\maize_phenology_20250224.csv', raw = False)\n",
    "Maize_set.drop_columns(['Unnamed: 9', 'Unnamed: 0'])\n",
    "Maize_set.phase_order_name([10, 12, 67, 65, 5, 6, 19, 20, 21, 24, ])\n",
    "\n",
    "Maize_set.get_mean_T('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\ERA5\\\\ERA5_land2_2011_2024.nc') #C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\tas_hyras_5_1951_2020_v5-0_de.nc')\n",
    "Maize_set.make_input_array(latlon_proj=True)\n",
    "#Maize_set.GDD_driver_data = Maize_set.GDD_driver_data.where(Maize_set.GDD_driver_data['time'] >= np.datetime64('2012-01-01'), drop = True)\n",
    "Maize_set.get_mean_T('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\ERA5\\\\ERA5_land2_2001_2010.nc')\n",
    "Maize_set.make_input_array(latlon_proj=True)\n",
    "Maize_set.phase_list = ['beginning of emergence', 'beginning of flowering', 'yellow ripeness']\n",
    "Maize_set.get_observed_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_locs = Maize_set.ds_observed.where(Maize_set.ds_observed['Referenzjahr'] >=2000).dropna()\n",
    "stat_locs = stat_locs[['lat', 'lon', 'Stations_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\Saved_files\\\\station_coords_MODIS.csv\",\n",
    "        stat_locs.values,\n",
    "        delimiter =\", \",\n",
    "        fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_temp_values_in_frame(driver_array, ds_observed, driver_variable, latlon_proj = True, phase_list = ['yellow ripeness'], SOS_offset = 0, station_locations = False):\n",
    "    observations_to_use = ds_observed[['Stations_id', 'Referenzjahr', 'WC SOS date']].where(ds_observed['Referenzjahr'] > 1999).dropna(how='all').drop_duplicates()\n",
    "    observations_to_use['WC SOS date'] += np.timedelta64(SOS_offset, 'D')\n",
    "    observations_to_use['SOS_year'] = observations_to_use['WC SOS date'].dt.year\n",
    "    observations_to_use = observations_to_use.drop_duplicates(subset = ['SOS_year', 'Stations_id'])\n",
    "    # make an indexing array to pull values from the array of temperatures\n",
    "    time_station = xr.Dataset.from_dataframe(observations_to_use)\n",
    "    time_station = time_station.rename({'index':'observation', 'WC SOS date':'time'})\n",
    "    #print(time_station)\n",
    "    if not(latlon_proj):\n",
    "        time_station['time'] += np.timedelta64(12, 'h')\n",
    "\n",
    "    ## Initiate development time storage object - a list with a value for all the stations, that will change over time and be stored in a list.\n",
    "    t_dev = np.zeros(time_station.sizes['observation']) #Continuous development time. When this passes through some thresholds then have change in phase.\n",
    "    dev_time_series = [t_dev.copy()]\n",
    "    ## Make sure driver dataset uses station id to index this dimension\n",
    "    try:\n",
    "        driver_array = driver_array.set_xindex(['Stations_id'])\n",
    "    except:\n",
    "        print('Couldn\\'t reset index for station')\n",
    "    \n",
    "    #Run model\n",
    "    for day in range(300):\n",
    "        print(day)\n",
    "        # Pull values for temperature out of data frame\n",
    "        driver_values = driver_array.sel(time_station[['Stations_id', 'time']])#[driver_variable]#.values \n",
    "        #print('sel function applied')\n",
    "        driver_frame_at_day = driver_values[[driver_variable, 'Stations_id', 'time']].to_pandas().reset_index().drop(['number', 'lon', 'lat', 'observation'], axis=1)\n",
    "        #print('converted to pandas frame')\n",
    "        if day == 0:\n",
    "            SOS_years = driver_frame_at_day['time'].dt.year\n",
    "            \n",
    "            #Referenzjahrs = driver_frame_at_day['time'].dt.year + (driver_frame_at_day['time'].dt.dayofyear > 180)\n",
    "        driver_frame_at_day['SOS_year'] = SOS_years #driver_frame_at_day['time'].dt.year\n",
    "        #print(driver_frame_at_day)\n",
    "        driver_frame_at_day = driver_frame_at_day.drop('time', axis=1)\n",
    "        driver_frame_at_day = driver_frame_at_day.rename(columns = {driver_variable:f'temperature at day {day}'})\n",
    "        #print(len(observations_to_use[['SOS_year', 'Stations_id']]), len(observations_to_use[['SOS_year', 'Stations_id']].drop_duplicates()),\n",
    "        #    len(driver_frame_at_day[['SOS_year', 'Stations_id']]), len(driver_frame_at_day[['SOS_year', 'Stations_id']].drop_duplicates()))\n",
    "        observations_to_use = observations_to_use.merge(driver_frame_at_day, on=['SOS_year', 'Stations_id'], how='inner')\n",
    "        #print(observations_to_use)\n",
    "        #print('merged')\n",
    "        time_station['time'] += np.timedelta64(1, 'D')\n",
    "    ds = observations_to_use.merge(ds_observed[['Referenzjahr', 'Stations_id'] + [f'observed time to {phase}' for phase in phase_list]]).drop_duplicates(subset = ['Referenzjahr', 'Stations_id'])\n",
    "    #return ds\n",
    "    ds = ds.dropna(subset = ['temperature at day 0'] + [f'observed time to {phase}' for phase in phase_list]).drop_duplicates()#\n",
    "    ds[[f'observed time to {phase}' for phase in phase_list]] = ds[[f'observed time to {phase}' for phase in phase_list]] + np.timedelta64(-SOS_offset, 'D')\n",
    "    if type(station_locations) != bool:\n",
    "        ds = get_station_locations(ds, station_locations)\n",
    "    return ds#, observations_to_use, driver_frame_at_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't reset index for station\n"
     ]
    }
   ],
   "source": [
    "#Maize_set_Africa.ds_observed = Maize_set_Africa.phen_data\n",
    "Maize_set.get_X_y_for_ML2(driver_variable = 't2m')\n",
    "Maize_set.driver_frame_for_ML.to_csv('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\results_for_comparing\\\\Maize_ML_data_t2m_60D_Offset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen_data_after_97 = Maize_set_Africa.phen_data.loc[Maize_set_Africa.phen_data['WC SOS date'] >= np.datetime64('1999-01-01')]\n",
    "ds = put_temp_values_in_frame(Maize_set_Africa.GDD_driver_data, phen_data_after_97, 't2m', phase_list = ['beginning of flowering'],\n",
    "                               SOS_offset=0, station_locations=False)#, dad, otu \n",
    "#ds1 = ds1.dropna(subset = ['temperature at day 0', 'observed time to yellow ripeness']).drop_duplicates()#\n",
    "ds['observed time to beginning of flowering'] = ds['observed time to beginning of flowering'] - ds['WC SOS date']\n",
    "ds = ds.where((ds['observed time to beginning of flowering'].dt.days > 0) & (ds['observed time to beginning of flowering'].dt.days < 200)).dropna()\n",
    "ds.to_csv('C:\\\\Users\\\\wlwc1989\\\\Documents\\\\Phenology_Test_Notebooks\\\\phenology_dwd\\\\results_for_comparing\\\\Maize_ML_data_Africa_t2m_WC_SOS.csv')\n",
    "#ds = ds1\n",
    "#ds1 = put_ERA5_in_array(ds1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phenology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
